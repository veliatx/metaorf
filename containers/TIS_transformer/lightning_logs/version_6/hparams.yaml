attn_dropout: 0.1
causal: false
decay_rate: 0.96
depth: 8
dim: 48
dim_head: 16
emb_dropout: 0.1
feature_redraw_interval: 1000
ff_chunks: 1
ff_dropout: 0.1
ff_glu: false
generalized_attention: true
heads: 8
kernel_fn: !!python/object:torch.nn.modules.activation.ReLU
  _backward_hooks: !!python/object/apply:collections.OrderedDict
  - []
  _backward_pre_hooks: !!python/object/apply:collections.OrderedDict
  - []
  _buffers: !!python/object/apply:collections.OrderedDict
  - []
  _forward_hooks: !!python/object/apply:collections.OrderedDict
  - []
  _forward_hooks_with_kwargs: !!python/object/apply:collections.OrderedDict
  - []
  _forward_pre_hooks: !!python/object/apply:collections.OrderedDict
  - []
  _forward_pre_hooks_with_kwargs: !!python/object/apply:collections.OrderedDict
  - []
  _is_full_backward_hook: null
  _load_state_dict_post_hooks: !!python/object/apply:collections.OrderedDict
  - []
  _load_state_dict_pre_hooks: !!python/object/apply:collections.OrderedDict
  - []
  _modules: !!python/object/apply:collections.OrderedDict
  - []
  _non_persistent_buffers_set: !!set {}
  _parameters: !!python/object/apply:collections.OrderedDict
  - []
  _state_dict_hooks: !!python/object/apply:collections.OrderedDict
  - []
  _state_dict_pre_hooks: !!python/object/apply:collections.OrderedDict
  - []
  inplace: false
  training: true
local_attn_heads: 5
local_window_size: 256
lr: 0.001
mask_frac: 0.85
max_seq_len: 50000
metrics: []
mlm: false
nb_features: 80
num_tokens: 8
rand_frac: 0.15
reversible: false
tie_embed: false
use_rezero: false
use_scalenorm: false
warmup_steps: 1500
x_ribo: false
x_seq: true
